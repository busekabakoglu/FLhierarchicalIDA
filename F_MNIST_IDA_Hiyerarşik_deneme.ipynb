{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "F-MNIST  IDA - Hiyerarşik deneme",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/busekabakoglu/FLhierarchicalIDA/blob/main/F_MNIST_IDA_Hiyerar%C5%9Fik_deneme.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKgyXytzUNIj"
      },
      "source": [
        "# !pip install syft\n",
        "#import syft # send olayini yapacak\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "import collections\n",
        "import numpy as np   # tf icin lazim\n",
        "import tensorflow as tf \n",
        "import copy \n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5Cw8mzKMtnT"
      },
      "source": [
        "num_clients = 10\n",
        "batch_size = 20\n",
        "learning_rate = 0.01\n",
        "decay_rate = 0.995\n",
        "local_epochs = 1\n",
        "federated_rounds = 300\n",
        "checkpoint_path = \"checkpoint.ckpt\"\n",
        "saving_frequency = 1\n",
        "is_balanced = True\n",
        "load_weights = False\n",
        "current_epoch = {}\n",
        "if os.path.exists(\"current_epoch.json\"):\n",
        "  with open('current_epoch.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "    current_epoch = data\n",
        "edge_server_metrics = {} \n",
        "model_transfers = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHGAl5IDG7xK"
      },
      "source": [
        "class Utilities:\n",
        "  @staticmethod\n",
        "  def add(model1, model2):\n",
        "    temp = np.multiply(model1, 0)\n",
        "    for i in range(len(model2)):\n",
        "        temp[i] = np.add(model1[i], model2[i])\n",
        "    return temp\n",
        "\n",
        "  @staticmethod\n",
        "  def subtract(model1, model2):\n",
        "    temp = np.multiply(model1, 0)\n",
        "    for i in range(len(model2)):\n",
        "        temp[i] = np.subtract(model1[i], model2[i] )\n",
        "    return temp\n",
        "\n",
        "  @staticmethod\n",
        "  def divide(model, number):\n",
        "    temp = np.multiply(model, 0)\n",
        "    for i in range(len(model)):\n",
        "        temp[i] = model[i]/number\n",
        "    return temp\n",
        "\n",
        "  @staticmethod\n",
        "  def flatten_weights(weights):\n",
        "    flat_ = []\n",
        "    for layer in weights:\n",
        "      flat_ = tf.concat([flat_, tf.reshape(layer , [-1, ]) ] , axis=0)\n",
        "    return flat_\n",
        "\n",
        "  @staticmethod\n",
        "  def find_avg_model(client_models, num_clients):\n",
        "    num_clients = num_clients * 1.0\n",
        "    total = np.multiply(client_models[0], 0)\n",
        "    for model in client_models:\n",
        "      total = Utilities.add(total, model)\n",
        "    return Utilities.divide(total, num_clients)\n",
        "\n",
        "  @staticmethod\n",
        "  def ida_normalization_factor(client_models, avg_model):\n",
        "    # Z = np.multiply( avg_model , 0 )\n",
        "    Z = 0.0\n",
        "    for model in client_models:\n",
        "      # temp = np.multiply( avg_model , 0 )\n",
        "      # Going through all layers of the model\n",
        "      diff = Utilities.subtract(avg_model, model)\n",
        "      distance = tf.norm(Utilities.flatten_weights(diff), ord=1)\n",
        "      inv_distance = np.divide(1.0, distance)\n",
        "      Z = np.add(Z, inv_distance)\n",
        "      \n",
        "    return Z\n",
        "  \n",
        "  @staticmethod\n",
        "  def ida_coefficient_of_model(client_model, avg_model, Z ):\n",
        "    #sub = np.multiply(avg_model, 0.0)\n",
        "    diff = Utilities.subtract(avg_model, client_model)\n",
        "    distance = tf.norm(Utilities.flatten_weights(diff) , ord=1)\n",
        "    inv_distance = np.divide(1.0, distance)\n",
        "    inv_distance = np.divide(inv_distance, Z)\n",
        "    return inv_distance\n",
        "\n",
        "  @staticmethod\n",
        "  def all_ida_coefficients(client_models, avg_model, Z ):\n",
        "    coeffs = []\n",
        "    for model in  client_models:\n",
        "      coeffs.append( Utilities.ida_coefficient_of_model( model , avg_model , Z ) )\n",
        "    return coeffs "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1da2G96x2jV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a527bee-b9fc-4115-dc19-23349534d9aa"
      },
      "source": [
        "# Buse Version\n",
        "# train_x = np.load(\"./mimic_benchmark/X_train.npy\")\n",
        "# train_y = np.load(\"./mimic_benchmark/y_train.npy\")\n",
        "# test_x = np.load(\"./mimic_benchmark/X_test.npy\")\n",
        "# test_y = np.load(\"./mimic_benchmark/y_test.npy\")\n",
        "\n",
        "# Barış Version\n",
        "# train_x = np.load(\"./drive/My Drive/X_train.npy\")\n",
        "# train_y = np.load(\"./drive/My Drive/y_train.npy\")\n",
        "# test_x = np.load(\"./drive/My Drive/X_test.npy\")\n",
        "# test_y = np.load(\"./drive/My Drive/y_test.npy\")\n",
        "\n",
        "fmnist_train, fmnist_test = tf.keras.datasets.fashion_mnist.load_data()\n",
        "train_x, train_y = fmnist_train  # global \n",
        "test_x, test_y = fmnist_test     # global  (x, y)\n",
        "train_x = train_x / 255.0\n",
        "test_x  = test_x  / 255.0 \n",
        "train_x = train_x.reshape((train_x.shape[0], 28, 28, 1))\n",
        "test_x = test_x.reshape((test_x.shape[0], 28, 28, 1))\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(test_x.shape)\n",
        "print(test_y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(60000,)\n",
            "(10000, 28, 28, 1)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Rpg02or2jYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa74801-ebe6-4b0f-faef-9e50984cd49d"
      },
      "source": [
        "num_train_data_point = len(train_y) # global\n",
        "INPUT_SHAPE = train_x.shape[1:]\n",
        "print(INPUT_SHAPE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vY9Mq2Q2jbD"
      },
      "source": [
        "# Shuffles 2 arrays side by side\n",
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "# For example we have 10 clients\n",
        "# We have to distribute the data to these clients\n",
        "\n",
        "\n",
        "# Balanced Case Using All of the Data (No reuse)\n",
        "def split_data(distribution_type, num_clients, data_amount = 600):\n",
        "  if distribution_type == \"BALANCED\":\n",
        "    return getBalancedData(num_clients)\n",
        "  elif distribution_type == \"IMBALANCED\":\n",
        "    return get_imbalanced_data(num_clients)\n",
        "  elif distribution_type == \"FIXED_SIZE_BALANCED_DATA\":\n",
        "    return get_fixed_amount_balanced_data(num_clients, data_amount)\n",
        "  elif distribution_type == \"SKEWED\":\n",
        "    return get_skewed_data(num_clients, data_amount)\n",
        "  elif distribution_type == \"IMBALANCED_AND_SKEWED\":\n",
        "    return get_imbalanced_and_skewed_data(num_clients, data_amount)\n",
        "\n",
        "def getBalancedData( num_clients ):\n",
        "  x,y = unison_shuffled_copies(train_x, train_y)\n",
        "  new_x = np.array_split(x, num_clients)\n",
        "  new_y = np.array_split(y, num_clients)  \n",
        "  return new_x, new_y\n",
        "\n",
        "# Imbalanced and Skewed Data\n",
        "def get_imbalanced_and_skewed_data(num_clients, data_amount = 60000):\n",
        "  train_index_list = [[], [], [], [], [], [], [], [], [], []]\n",
        "  new_x = []\n",
        "  new_y = []\n",
        "  for i, v in enumerate(train_y):\n",
        "    train_index_list[v].append(i)\n",
        "\n",
        "  for i in range(num_clients):\n",
        "    cur_x, cur_y = make_split_train_data_by_number(i, train_index_list, np.random.randint(1, high=len(train_index_list[i]), dtype=int))\n",
        "    new_x.append(cur_x)\n",
        "    new_y.append(cur_y)\n",
        "\n",
        "  return new_x, new_y\n",
        "\n",
        "# Skewed Distribution\n",
        "def get_skewed_data(num_clients, max_size):\n",
        "  train_index_list = [[], [], [], [], [], [], [], [], [], []]\n",
        "  new_x = []\n",
        "  new_y = []\n",
        "  for i, v in enumerate(train_y):\n",
        "    train_index_list[v].append(i)\n",
        "  \n",
        "  for i in range(len(train_index_list)):\n",
        "    print(len(train_index_list[i]))\n",
        "\n",
        "  for i in range(num_clients):\n",
        "    cur_x = []\n",
        "    cur_y = []\n",
        "    for index in train_index_list[i]:\n",
        "      cur_x.append(train_x[index])\n",
        "      cur_y.append(train_y[index])\n",
        "    #cur_x, cur_y = (make_split_train_data_by_number(i, train_index_list, max_size))\n",
        "    new_x.append(np.array(cur_x))\n",
        "    new_y.append(np.array(cur_y))\n",
        "\n",
        "  return new_x, new_y\n",
        " \n",
        "def make_split_train_data_by_number(index_number, train_index_list, size=600):\n",
        "    if index_number != -1 :\n",
        "        random_index = np.random.randint(0, high=len(train_index_list[index_number]), size=min(size, len(train_index_list[index_number])))\n",
        "        s_train_x = []\n",
        "        s_train_y = []\n",
        "        for v in random_index:\n",
        "            s_train_x.append(train_x[train_index_list[index_number][v]])\n",
        "            s_train_y.append(train_y[train_index_list[index_number][v]])\n",
        "        return s_train_x, s_train_y\n",
        "    else:\n",
        "        return train_x, train_y\n",
        "\n",
        "# Balanced data \n",
        "def get_fixed_amount_balanced_data(num_clients, data_amount):\n",
        "  x,y = unison_shuffled_copies(train_x, train_y)\n",
        "  x = x[:num_clients*data_amount]\n",
        "  y = y[:num_clients*data_amount]\n",
        "  new_x = np.array_split(x, num_clients)\n",
        "  new_y = np.array_split(y, num_clients)  \n",
        "  return new_x, new_y\n",
        "\n",
        "# Imbalanced Data Distribution\n",
        "def get_imbalanced_data(num_clients):\n",
        "  random_data_amounts = np.random.randint(1, high=num_train_data_point//num_clients, size=num_clients, dtype=int)\n",
        "  print(random_data_amounts)\n",
        "  x,y = unison_shuffled_copies(train_x, train_y)\n",
        "  start = 0\n",
        "  new_x = []\n",
        "  new_y = []\n",
        "  for amount in random_data_amounts:\n",
        "    end = start + amount\n",
        "    current_x = x[start:end]\n",
        "    current_y = y[start:end]\n",
        "    start = end\n",
        "    new_x.append(current_x)\n",
        "    new_y.append(current_y)\n",
        "  return new_x, new_y\n",
        "\n",
        "# To be used for splitting the data\n",
        "#fed_x , fed_y = getBalancedData(num_clients)\n",
        "# fed_x , fed_y = split_data(\"BALANCED\", num_clients)\n",
        "# fed_x = np.array(fed_x)\n",
        "# fed_y = np.array(fed_y)\n",
        "# for y in fed_y:\n",
        "#   print(len(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKzPRICD2jdZ"
      },
      "source": [
        "def getModel():\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Conv2D(\n",
        "                                  filters= 6,\n",
        "                                  kernel_size = (5,5),\n",
        "                                  strides=(1,1),\n",
        "                                  activation='tanh',\n",
        "                                  input_shape=INPUT_SHAPE))\n",
        "  \n",
        "  model.add(tf.keras.layers.AveragePooling2D(pool_size=(2,2),\n",
        "                                          strides=(2,2)))\n",
        "  \n",
        "  model.add(tf.keras.layers.Conv2D(\n",
        "                                  filters=16,\n",
        "                                  kernel_size = (5,5),\n",
        "                                  strides=(1,1),\n",
        "                                  activation='tanh'))\n",
        "  \n",
        "  model.add(tf.keras.layers.AveragePooling2D(pool_size=(2,2),\n",
        "                                          strides=(2,2)))\n",
        "  \n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  \n",
        "  model.add(tf.keras.layers.Dense(units=120,activation='tanh'))\n",
        "  \n",
        "  model.add(tf.keras.layers.Dense(units=84,activation='tanh'))\n",
        "    \n",
        "  model.add(tf.keras.layers.Dense(units=10,activation='softmax'))\n",
        "  return model\n",
        "\n",
        "def visualize_metrics(metrics, Loss=True):\n",
        "      # Get training and test loss histories\n",
        "  count = 0\n",
        "  acc = metrics[\"acc\"]\n",
        "  loss = metrics[\"loss\"]\n",
        "  \n",
        "  # # Create count of the number of epochs\n",
        "  epoch_count = range(1, len(acc) + 1)\n",
        "  # Visualize loss history\n",
        "  if Loss:\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.plot(epoch_count, loss, 'r--')\n",
        "    plt.legend(['Training Loss'])\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "  plt.subplot(2,1,2)\n",
        "  plt.plot(epoch_count, acc, 'b--')\n",
        "  plt.legend(['Accuracy'])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Acc')\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.show()\n",
        "\n",
        "def visualize_edge_metrics(server, Loss=False):\n",
        "  for key in edge_server_metrics.keys():\n",
        "    visualize_metrics(edge_server_metrics[key], Loss=Loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BibHjSC-J2V"
      },
      "source": [
        "class Client():\n",
        "  def __init__(self, id, x, y):\n",
        "    self.id = id\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.amount_of_data = len(y)\n",
        "    self.model = getModel()\n",
        "    self.old_model = []\n",
        "    self.print_data_points()\n",
        "\n",
        "  def print_data_points(self):\n",
        "    print(\"CLIENT \", self.id, \"-------------------\")\n",
        "    print(\"data point number: \", len(self.x))\n",
        "    print(Counter(self.y))\n",
        "\n",
        "  def init_session(self, global_model):\n",
        "    self.model.set_weights( global_model )\n",
        "\n",
        "  def train_client( self, learning_rate, batch_size, local_epochs, global_model ):\n",
        "    model_transfers += 1\n",
        "    #old_model = np.array( self.model.get_weights(), dtype=object )\n",
        "    updated_model = copy.deepcopy(global_model) \n",
        "    self.model.set_weights( updated_model )\n",
        "    optimizer = tf.keras.optimizers.SGD( lr=learning_rate )\n",
        "    self.model.compile( loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"] )\n",
        "    self.model.fit( self.x, self.y, epochs=local_epochs, batch_size=batch_size, shuffle=True, verbose=1 )\n",
        "    new_model = np.array( self.model.get_weights() , dtype=object )\n",
        "    #gradients = np.subtract( new_model , old_model )\n",
        "    model_transfers += 1\n",
        "    return new_model\n",
        "\n",
        "class EdgeServer():\n",
        "  def __init__( self, client_list, client_indexes, global_model, no):\n",
        "    self.client_list = client_list\n",
        "    self.client_indexes = client_indexes\n",
        "    self.gradients = np.multiply( np.array(global_model ,dtype=object), 0 )\n",
        "    self.model_weights = np.array(global_model ,dtype=object)\n",
        "    self.num_clients = len(client_indexes)\n",
        "    try:\n",
        "      dummy = edge_server_metrics[str(no)]['loss'][1]\n",
        "    except:\n",
        "      edge_server_metrics[str(no)] = {\"loss\" : [], \"acc\" :[], \"prec\" :[] , \"recall\" : [] }\n",
        "    self.model = getModel()\n",
        "    self.no = no\n",
        "    \n",
        "  \n",
        "  def train_slaves(self, learning_rate, batch_size, local_epochs, edge_interval, global_interval): # :)\n",
        "    model_transfers += 1\n",
        "    for round in range(global_interval):\n",
        "      returned_models_total = np.multiply(self.gradients, 0)\n",
        "      client_models = []\n",
        "\n",
        "      for client in self.client_list:\n",
        "        client_model =  client.train_client(learning_rate, batch_size, edge_interval, self.model_weights)\n",
        "        returned_models_total = np.add( returned_models_total, client_model ) \n",
        "        client_models.append(client_model)\n",
        "\n",
        "      avg_model = np.divide( returned_models_total , self.num_clients )\n",
        "      Z = Utilities.ida_normalization_factor(client_models, avg_model)\n",
        "      ida_factors = Utilities.all_ida_coefficients(client_models, avg_model, Z)\n",
        "      weighted_total_client_models = np.multiply(self.gradients, 0)\n",
        "\n",
        "      for i in range(len(client_models)):\n",
        "        weighted_total_client_models = np.add(weighted_total_client_models, np.multiply( client_models[i] , ida_factors[i] ) )\n",
        "\n",
        "      self.model_weights = weighted_total_client_models\n",
        "\n",
        "      self.model.set_weights(self.model_weights)\n",
        "\n",
        "      optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
        "      self.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,metrics=[\"accuracy\", \n",
        "              #  tf.keras.metrics.AUC(curve='PR', multi_label=True),\n",
        "              #  tf.keras.metrics.AUC(curve=\"ROC\"),\n",
        "              #  tf.keras.metrics.Precision(top_k=1),\n",
        "              #  tf.keras.metrics.Recall(top_k=1),\n",
        "              #  tf.keras.metrics.FalseNegatives(),\n",
        "              #  tf.keras.metrics.FalsePositives(),\n",
        "               ])\n",
        "      print( \"EDGE SERVER {} METRICS : \".format(self.no) )\n",
        "      current_state = self.model.evaluate( test_x , test_y )\n",
        "      \n",
        "      edge_server_metrics[str(self.no)]['loss'].append(current_state[0])\n",
        "      edge_server_metrics[str(self.no)]['acc'].append(current_state[1])\n",
        "      # edge_server_metrics[str(self.no)][\"prec\"].append(current_state[2])\n",
        "      # edge_server_metrics[str(self.no)][\"recall\"].append(current_state[3])\n",
        "\n",
        "    model_transfers += 1\n",
        "    return self.model_weights\n",
        "    \n",
        "class Server():\n",
        "  def __init__(self, num_clients, num_edge_servers, distribution, edge_interval, global_interval, load_weights=False, visualization_freq=30 ):\n",
        "    self.history = {\"loss\" : [], \"acc\" :[] }\n",
        "    self.model = getModel()\n",
        "    self.num_clients = num_clients\n",
        "    self.num_edge_servers = num_edge_servers\n",
        "    self.gradients = np.multiply( np.array(self.model.get_weights() ,dtype=object), 0 )\n",
        "    self.clients = []\n",
        "    self.edge_servers = []\n",
        "    self.visualization_freq = visualization_freq\n",
        "    self.curr_epoch = 0\n",
        "    self.distribution = distribution\n",
        "    self.edge_interval = edge_interval\n",
        "    self.global_interval = global_interval\n",
        "    self.init_clients()\n",
        "    self.init_edge_servers()\n",
        "\n",
        "  def train_slave_masters(self, learning_rate, batch_size, local_epochs, edge_interval, global_interval, federated_rounds, decay_rate):\n",
        "    # self.init_clients()\n",
        "    # self.init_edge_servers()\n",
        "\n",
        "    for round in range(federated_rounds):\n",
        "      returned_models = np.multiply(self.gradients, 0)\n",
        "      edge_models = [] \n",
        "      if round % 4 == 0 :\n",
        "        self.clear_backend()\n",
        "      \n",
        "      for edge_server in self.edge_servers: \n",
        "        edge_model = edge_server.train_slaves(learning_rate, batch_size, local_epochs, edge_interval, global_interval) \n",
        "        returned_models = np.add(returned_models, edge_model)\n",
        "        edge_models.append(edge_model)\n",
        "      \n",
        "      avg_model = np.divide( returned_models, len(self.edge_servers) )\n",
        "      Z = Utilities.ida_normalization_factor(edge_models, avg_model)\n",
        "      ida_factors = Utilities.all_ida_coefficients(edge_models, avg_model, Z)\n",
        "      updated_model = np.multiply(self.gradients, 0)\n",
        "      for i in range(len(edge_models)):\n",
        "        updated_model = np.add(updated_model, np.multiply( edge_models[i] , ida_factors[i] ) )\n",
        "      self.model.set_weights(updated_model)\n",
        "      optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
        "      self.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "      print(\"\"\".... EPOCH {} DONE ....\"\"\".format(round))\n",
        "      current_state = self.model.evaluate( train_x, train_y )\n",
        "      self.model.evaluate( test_x , test_y )\n",
        "      self.history['loss'].append(current_state[0])\n",
        "      self.history['acc'].append(current_state[1])\n",
        "      if round != 0 and round % self.visualization_freq == 0 : \n",
        "        visualize_metrics(self.history)\n",
        "      learning_rate = learning_rate  * decay_rate\n",
        "      \n",
        "  def init_clients(self):\n",
        "    weight_factors = []\n",
        "    total_data = 0\n",
        "    for i in range(self.num_clients):\n",
        "      self.clients.append( Client( i, fed_x[ i ], fed_y[ i ] ) )\n",
        "    \n",
        "    initial_model = np.array( self.model.get_weights(), dtype=object )\n",
        "    for client in self.clients:\n",
        "      client.init_session(initial_model)\n",
        "      #burda niye amount of data eklemişiz :\n",
        "      # FedAvg'dan kalma,  Gerek yok aslinda ama zararı da yok bundan sonraki 2 satırın\n",
        "      weight_factors.append(client.amount_of_data)\n",
        "      total_data += client.amount_of_data\n",
        "    self.weight_factors = np.array(weight_factors) / float(total_data)\n",
        "\n",
        "  def init_edge_servers(self):\n",
        "    edge = self.num_edge_servers\n",
        "    client_number = self.num_clients//self.num_edge_servers\n",
        "    for i in range(self.num_edge_servers):\n",
        "      self.edge_servers.append( EdgeServer( [], [], self.model.get_weights(), i+1 ) )\n",
        "    for i in range(self.num_clients):\n",
        "      edge_index = i//client_number\n",
        "      self.edge_servers[edge_index].client_list.append(self.clients[i])\n",
        "      self.edge_servers[edge_index].client_indexes.append(i)\n",
        "      self.edge_servers[edge_index].num_clients += 1\n",
        "    print( \"... EDGE SERVERS INITIALIZED ...\" )\n",
        "      \n",
        "  \n",
        "\n",
        "  def clear_backend(self):\n",
        "    print(\"CLEARING BACKEND....\")\n",
        "    saved_weights = self.model.get_weights() \n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "    self.model.set_weights(saved_weights)\n",
        "    del self.clients\n",
        "    self.clients = []\n",
        "    del self.edge_servers\n",
        "    self.edge_servers = []\n",
        "    self.init_clients()\n",
        "    self.init_edge_servers() \n",
        "    # for client in self.clients:\n",
        "      # client.model.set_weights(saved_weights)\n",
        "    print(\"BACKEND CLEARED!\")\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV8rZdfXM8Qe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b5fe5d91-61de-45df-da39-fa52c2316b98"
      },
      "source": [
        "# BALANCED CASE\n",
        "distribution = \"BALANCED\"\n",
        "fed_x , fed_y = split_data(distribution, num_clients)\n",
        "fed_x = np.array(fed_x)\n",
        "fed_y = np.array(fed_y)\n",
        "\n",
        "server1 = Server( num_clients, 2, distribution, edge_interval=1, global_interval=3, load_weights=False, visualization_freq=30)\n",
        "# server1.model.summary()\n",
        "server1.train_slave_masters(learning_rate, batch_size, local_epochs, 2, 3, 50, decay_rate)\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
        "server1.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "print(\"TEST SET EVALUATION\")\n",
        "print(server1.model.evaluate(test_x , test_y))\n",
        "visualize_metrics(server1.history)\n",
        "visualize_edge_metrics(1)\n",
        "print(\"HISTORY : \")\n",
        "print(server1.model.history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CLIENT  0 -------------------\n",
            "data point number:  6000\n",
            "Counter({0: 647, 5: 627, 4: 622, 6: 612, 2: 606, 7: 594, 8: 583, 1: 577, 3: 572, 9: 560})\n",
            "CLIENT  1 -------------------\n",
            "data point number:  6000\n",
            "Counter({9: 619, 7: 614, 8: 614, 3: 610, 0: 609, 4: 606, 2: 598, 1: 589, 5: 582, 6: 559})\n",
            "CLIENT  2 -------------------\n",
            "data point number:  6000\n",
            "Counter({2: 636, 1: 636, 4: 604, 7: 599, 0: 598, 3: 595, 6: 594, 5: 586, 9: 580, 8: 572})\n",
            "CLIENT  3 -------------------\n",
            "data point number:  6000\n",
            "Counter({5: 621, 8: 620, 6: 612, 9: 608, 1: 600, 7: 598, 2: 596, 3: 591, 4: 584, 0: 570})\n",
            "CLIENT  4 -------------------\n",
            "data point number:  6000\n",
            "Counter({5: 639, 7: 626, 6: 615, 0: 612, 4: 602, 1: 592, 3: 587, 9: 577, 8: 576, 2: 574})\n",
            "CLIENT  5 -------------------\n",
            "data point number:  6000\n",
            "Counter({9: 662, 1: 611, 5: 608, 7: 608, 3: 602, 8: 595, 2: 593, 0: 580, 6: 572, 4: 569})\n",
            "CLIENT  6 -------------------\n",
            "data point number:  6000\n",
            "Counter({1: 641, 8: 636, 3: 632, 2: 623, 9: 612, 6: 591, 4: 590, 7: 569, 5: 560, 0: 546})\n",
            "CLIENT  7 -------------------\n",
            "data point number:  6000\n",
            "Counter({6: 664, 4: 618, 0: 608, 7: 605, 5: 602, 2: 590, 1: 587, 8: 580, 3: 577, 9: 569})\n",
            "CLIENT  8 -------------------\n",
            "data point number:  6000\n",
            "Counter({2: 618, 9: 614, 4: 608, 7: 608, 0: 607, 3: 606, 6: 589, 5: 588, 8: 587, 1: 575})\n",
            "CLIENT  9 -------------------\n",
            "data point number:  6000\n",
            "Counter({8: 637, 3: 628, 0: 623, 9: 599, 4: 597, 6: 592, 1: 592, 5: 587, 7: 579, 2: 566})\n",
            "... EDGE SERVERS INITIALIZED ...\n",
            "CLEARING BACKEND....\n",
            "CLIENT  0 -------------------\n",
            "data point number:  6000\n",
            "Counter({0: 647, 5: 627, 4: 622, 6: 612, 2: 606, 7: 594, 8: 583, 1: 577, 3: 572, 9: 560})\n",
            "CLIENT  1 -------------------\n",
            "data point number:  6000\n",
            "Counter({9: 619, 7: 614, 8: 614, 3: 610, 0: 609, 4: 606, 2: 598, 1: 589, 5: 582, 6: 559})\n",
            "CLIENT  2 -------------------\n",
            "data point number:  6000\n",
            "Counter({2: 636, 1: 636, 4: 604, 7: 599, 0: 598, 3: 595, 6: 594, 5: 586, 9: 580, 8: 572})\n",
            "CLIENT  3 -------------------\n",
            "data point number:  6000\n",
            "Counter({5: 621, 8: 620, 6: 612, 9: 608, 1: 600, 7: 598, 2: 596, 3: 591, 4: 584, 0: 570})\n",
            "CLIENT  4 -------------------\n",
            "data point number:  6000\n",
            "Counter({5: 639, 7: 626, 6: 615, 0: 612, 4: 602, 1: 592, 3: 587, 9: 577, 8: 576, 2: 574})\n",
            "CLIENT  5 -------------------\n",
            "data point number:  6000\n",
            "Counter({9: 662, 1: 611, 5: 608, 7: 608, 3: 602, 8: 595, 2: 593, 0: 580, 6: 572, 4: 569})\n",
            "CLIENT  6 -------------------\n",
            "data point number:  6000\n",
            "Counter({1: 641, 8: 636, 3: 632, 2: 623, 9: 612, 6: 591, 4: 590, 7: 569, 5: 560, 0: 546})\n",
            "CLIENT  7 -------------------\n",
            "data point number:  6000\n",
            "Counter({6: 664, 4: 618, 0: 608, 7: 605, 5: 602, 2: 590, 1: 587, 8: 580, 3: 577, 9: 569})\n",
            "CLIENT  8 -------------------\n",
            "data point number:  6000\n",
            "Counter({2: 618, 9: 614, 4: 608, 7: 608, 0: 607, 3: 606, 6: 589, 5: 588, 8: 587, 1: 575})\n",
            "CLIENT  9 -------------------\n",
            "data point number:  6000\n",
            "Counter({8: 637, 3: 628, 0: 623, 9: 599, 4: 597, 6: 592, 1: 592, 5: 587, 7: 579, 2: 566})\n",
            "... EDGE SERVERS INITIALIZED ...\n",
            "BACKEND CLEARED!\n",
            "Epoch 1/2\n",
            "300/300 [==============================] - 1s 2ms/step - loss: 1.9206 - accuracy: 0.3726\n",
            "Epoch 2/2\n",
            "300/300 [==============================] - 1s 2ms/step - loss: 0.9965 - accuracy: 0.6613\n",
            "Epoch 1/2\n",
            "300/300 [==============================] - 1s 2ms/step - loss: 1.9089 - accuracy: 0.3830\n",
            "Epoch 2/2\n",
            "300/300 [==============================] - 1s 2ms/step - loss: 0.9714 - accuracy: 0.6847\n",
            "Epoch 1/2\n",
            "278/300 [==========================>...] - ETA: 0s - loss: 1.9337 - accuracy: 0.3507"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-168-9d496f051fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mserver1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mServer\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnum_clients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualization_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# server1.model.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mserver1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_slave_masters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-167-b02a4227416f>\u001b[0m in \u001b[0;36mtrain_slave_masters\u001b[0;34m(self, learning_rate, batch_size, local_epochs, edge_interval, global_interval, federated_rounds, decay_rate)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0medge_server\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_servers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0medge_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_slaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mreturned_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturned_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0medge_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-167-b02a4227416f>\u001b[0m in \u001b[0;36mtrain_slaves\u001b[0;34m(self, learning_rate, batch_size, local_epochs, edge_interval, global_interval)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mclient_model\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mreturned_models_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreturned_models_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_model\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mclient_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-167-b02a4227416f>\u001b[0m in \u001b[0;36mtrain_client\u001b[0;34m(self, learning_rate, batch_size, local_epochs, global_model)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#gradients = np.subtract( new_model , old_model )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVLQWhcubqaj"
      },
      "source": [
        "# SKEWED CASE\n",
        "distribution = \"SKEWED\"\n",
        "fed_x , fed_y = split_data(distribution, num_clients)\n",
        "fed_x = np.array(fed_x)\n",
        "fed_y = np.array(fed_y)\n",
        "\n",
        "server1 = Server(num_clients, distribution, load_weights=False)\n",
        "server1.init_clients()\n",
        "server1.model.summary()\n",
        "server1.train_slaves(learning_rate, batch_size, local_epochs, federated_rounds)\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
        "server1.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "print(\"TEST SET EVALUATION\")\n",
        "print(server1.model.evaluate(test_x , test_y))\n",
        "visualize_metrics(server1.history)\n",
        "\n",
        "print(\"HISTORY : \")\n",
        "print(server1.model.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3WT1huq2jfe"
      },
      "source": [
        "optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
        "server.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "a = server.model.evaluate(test_x , test_y)\n",
        "history = server.model.history\n",
        "print(a)\n",
        "print(history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkcIGsJd2jh4"
      },
      "source": [
        "# server.train_slaves(0.001, batch_size, local_epochs, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L73YfswG2jj-"
      },
      "source": [
        "# IMBALANCED CASE\n",
        "distribution = \"FIXED_SIZE_BALANCED_DATA\"\n",
        "fed_x , fed_y = split_data(distribution, num_clients)\n",
        "fed_x = np.array(fed_x)\n",
        "fed_y = np.array(fed_y)\n",
        "\n",
        "server2 = Server(num_clients, distribution, load_weights=load_weights)\n",
        "server2.init_clients()\n",
        "server2.model.summary()\n",
        "server2.train_slaves(learning_rate, batch_size, local_epochs, federated_rounds)\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
        "server2.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "print(\"TEST SET EVALUATION\")\n",
        "print(server2.model.evaluate(test_x , test_y))\n",
        "visualize_metrics(server2.history)\n",
        "\n",
        "print(\"HISTORY : \")\n",
        "print(server2.model.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-aGYxcVA6uz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbnkuKDU2jmW"
      },
      "source": [
        "# a = getModel()\n",
        "# a.compile(optimizer=tf.optimizers.SGD(), loss=\"sparse_categorical_crossentropy\")\n",
        "# a.fit(train_x, train_y, epochs=1, shuffle=True, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggm6WPxI2jov"
      },
      "source": [
        "shapes = [\n",
        "          [784, 10],\n",
        "          [10]\n",
        "]\n",
        "aa = np.array(server.model.get_weights(), dtype=object)\n",
        "\n",
        "def flatten_weights(weights, shape=shapes):\n",
        "  flat_ = []\n",
        "  for layer in weights:\n",
        "    flat_ = tf.concat([flat_, tf.reshape(layer , [-1, ]) ] , axis=0)\n",
        "  return flat_\n",
        "l = flatten_weights(aa)\n",
        "\n",
        "b = tf.Variable(l, trainable=True, dtype=tf.float32)\n",
        "\n",
        "tf.norm(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNvi5ez32jq7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIMIC-III IDA - Hiyerarşik deneme",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/busekabakoglu/FLhierarchicalIDA/blob/main/MIMIC_III_IDA_Hiyerar%C5%9Fik_deneme.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKgyXytzUNIj"
      },
      "source": [
        " # !pip install syft\n",
        "#import syft # send olayini yapacak\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "import collections\n",
        "import numpy as np   # tf icin lazim\n",
        "import tensorflow as tf \n",
        "import copy \n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.utils import shuffle\n",
        "from collections import Counter"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5Cw8mzKMtnT"
      },
      "source": [
        "num_clients = 10\n",
        "batch_size = 8\n",
        "learning_rate = 0.01\n",
        "decay_rate = 0.995\n",
        "local_epochs = 1\n",
        "federated_rounds = 30\n",
        "checkpoint_path = \"checkpoint.ckpt\"\n",
        "saving_frequency = 1\n",
        "is_balanced = True\n",
        "load_weights = False\n",
        "current_epoch = {}\n",
        "if os.path.exists(\"current_epoch.json\"):\n",
        "  with open('current_epoch.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "    current_epoch = data"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHGAl5IDG7xK"
      },
      "source": [
        "class Utilities:\n",
        "  @staticmethod\n",
        "  def add(model1, model2):\n",
        "    temp = np.multiply(model1, 0)\n",
        "    for i in range(len(model2)):\n",
        "        temp[i] = np.add(model1[i], model2[i])\n",
        "    return temp\n",
        "\n",
        "  @staticmethod\n",
        "  def subtract(model1, model2):\n",
        "    temp = np.multiply(model1, 0)\n",
        "    for i in range(len(model2)):\n",
        "        temp[i] = np.subtract(model1[i], model2[i] )\n",
        "    return temp\n",
        "\n",
        "  @staticmethod\n",
        "  def divide(model, number):\n",
        "    temp = np.multiply(model, 0)\n",
        "    for i in range(len(model)):\n",
        "        temp[i] = model[i]/number\n",
        "    return temp\n",
        "\n",
        "  @staticmethod\n",
        "  def flatten_weights(weights):\n",
        "    flat_ = []\n",
        "    for layer in weights:\n",
        "      flat_ = tf.concat([flat_, tf.reshape(layer , [-1, ]) ] , axis=0)\n",
        "    return flat_\n",
        "\n",
        "  @staticmethod\n",
        "  def find_avg_model(client_models, num_clients):\n",
        "    num_clients = num_clients * 1.0\n",
        "    total = np.multiply(client_models[0], 0)\n",
        "    for model in client_models:\n",
        "      total = Utilities.add(total, model)\n",
        "    return Utilities.divide(total, num_clients)\n",
        "\n",
        "  @staticmethod\n",
        "  def ida_normalization_factor(client_models, avg_model):\n",
        "    # Z = np.multiply( avg_model , 0 )\n",
        "    Z = 0.0\n",
        "    for model in client_models:\n",
        "      # temp = np.multiply( avg_model , 0 )\n",
        "      # Going through all layers of the model\n",
        "      diff = Utilities.subtract(avg_model, model)\n",
        "      distance = tf.norm(Utilities.flatten_weights(diff), ord=1)\n",
        "      inv_distance = np.divide(1.0, distance)\n",
        "      Z = np.add(Z, inv_distance)\n",
        "      \n",
        "    return Z\n",
        "  \n",
        "  @staticmethod\n",
        "  def ida_coefficient_of_model(client_model, avg_model, Z ):\n",
        "    #sub = np.multiply(avg_model, 0.0)\n",
        "    diff = Utilities.subtract(avg_model, client_model)\n",
        "    distance = tf.norm(Utilities.flatten_weights(diff) , ord=1)\n",
        "    inv_distance = np.divide(1.0, distance)\n",
        "    inv_distance = np.divide(inv_distance, Z)\n",
        "    return inv_distance\n",
        "\n",
        "  @staticmethod\n",
        "  def all_ida_coefficients(client_models, avg_model, Z ):\n",
        "    coeffs = []\n",
        "    for model in  client_models:\n",
        "      coeffs.append( Utilities.ida_coefficient_of_model( model , avg_model , Z ) )\n",
        "    return coeffs "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxrpocRJmMJC"
      },
      "source": [
        "# Burayi bak\n",
        "\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !unzip mimic_benchmark.zip -d .\n",
        "# drive.mount('/content/drive')\n",
        "# !unzip \"/content/drive/My Drive/mimic/mimic_benchmark\" -d ."
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhG198aW0WQu",
        "outputId": "fcdd9f0b-21f5-4352-e795-683d961fdbb3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1da2G96x2jV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c5e3655-cee5-412e-86b5-1f7c97fd6fb9"
      },
      "source": [
        "# Buse Version\n",
        "train_x = np.load(\"./mimic_benchmark/X_train.npy\")\n",
        "train_y = np.load(\"./mimic_benchmark/y_train.npy\")\n",
        "test_x = np.load(\"./mimic_benchmark/X_test.npy\")\n",
        "test_y = np.load(\"./mimic_benchmark/y_test.npy\")\n",
        "\n",
        "# Barış Version\n",
        "# train_x = np.load(\"./drive/My Drive/X_train.npy\")\n",
        "# train_y = np.load(\"./drive/My Drive/y_train.npy\")\n",
        "# test_x = np.load(\"./drive/My Drive/X_test.npy\")\n",
        "# test_y = np.load(\"./drive/My Drive/y_test.npy\")\n",
        "\n",
        "\n",
        "train_x, train_y = shuffle(train_x, train_y, random_state=0)\n",
        "test_x, test_y = shuffle(test_x, test_y, random_state=0)\n",
        "\n",
        "train_y = np.argmax(train_y, axis=1)\n",
        "test_y =  np.argmax(test_y, axis=1)\n",
        "train_y = np.reshape(train_y, [-1, ]) \n",
        "test_y =  np.reshape(test_y, [-1, ])\n",
        "train_y = np.asarray(train_y).reshape((-1,1))\n",
        "test_y = np.asarray(test_y).astype('float32').reshape((-1,1))\n",
        "\n",
        "\n",
        "# fmnist_train, fmnist_test = tf.keras.datasets.fashion_mnist.load_data()\n",
        "# train_x, train_y = fmnist_train  # global \n",
        "# test_x, test_y = fmnist_test     # global  (x, y)\n",
        "# train_x = train_x / 255.0\n",
        "# test_x  = test_x  / 255.0 \n",
        "# train_x = train_x.reshape((train_x.shape[0], 28, 28, 1))\n",
        "# test_x = test_x.reshape((test_x.shape[0], 28, 28, 1))\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(test_x.shape)\n",
        "print(test_y.shape)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14681, 48, 76)\n",
            "(14681, 1)\n",
            "(3236, 48, 76)\n",
            "(3236, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Rpg02or2jYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ee567ee-8aa1-4ac2-9736-e09276838582"
      },
      "source": [
        "num_train_data_point = len(train_y) # global\n",
        "INPUT_SHAPE = train_x.shape[1:]\n",
        "print(INPUT_SHAPE)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(48, 76)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vY9Mq2Q2jbD"
      },
      "source": [
        "# Shuffles 2 arrays side by side\n",
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "# For example we have 10 clients\n",
        "# We have to distribute the data to these clients\n",
        "\n",
        "\n",
        "# Balanced Case Using All of the Data (No reuse)\n",
        "def split_data(distribution_type, num_clients, data_amount = 600):\n",
        "  if distribution_type == \"BALANCED\":\n",
        "    return getBalancedData(num_clients)\n",
        "  elif distribution_type == \"IMBALANCED\":\n",
        "    return get_imbalanced_data(num_clients)\n",
        "  elif distribution_type == \"FIXED_SIZE_BALANCED_DATA\":\n",
        "    return get_fixed_amount_balanced_data(num_clients, data_amount)\n",
        "  elif distribution_type == \"SKEWED\":\n",
        "    return get_skewed_data(num_clients, data_amount)\n",
        "  elif distribution_type == \"IMBALANCED_AND_SKEWED\":\n",
        "    return get_imbalanced_and_skewed_data(num_clients, data_amount)\n",
        "\n",
        "def getBalancedData( num_clients ):\n",
        "  x,y = unison_shuffled_copies(train_x, train_y)\n",
        "  new_x = np.array_split(x, num_clients)\n",
        "  new_y = np.array_split(y, num_clients)  \n",
        "  return new_x, new_y\n",
        "\n",
        "# Imbalanced and Skewed Data\n",
        "def get_imbalanced_and_skewed_data(num_clients, data_amount = 60000):\n",
        "  train_index_list = [[], [], [], [], [], [], [], [], [], []]\n",
        "  new_x = []\n",
        "  new_y = []\n",
        "  for i, v in enumerate(train_y):\n",
        "    train_index_list[v].append(i)\n",
        "\n",
        "  for i in range(num_clients):\n",
        "    cur_x, cur_y = make_split_train_data_by_number(i, train_index_list, np.random.randint(1, high=len(train_index_list[i]), dtype=int))\n",
        "    new_x.append(cur_x)\n",
        "    new_y.append(cur_y)\n",
        "\n",
        "  return new_x, new_y\n",
        "\n",
        "# Skewed Distribution\n",
        "def get_skewed_data(num_clients, max_size):\n",
        "  train_index_list = [[], [], [], [], [], [], [], [], [], []]\n",
        "  new_x = []\n",
        "  new_y = []\n",
        "  for i, v in enumerate(train_y):\n",
        "    train_index_list[v].append(i)\n",
        "  \n",
        "  for i in range(len(train_index_list)):\n",
        "    print(len(train_index_list[i]))\n",
        "\n",
        "  for i in range(num_clients):\n",
        "    cur_x = []\n",
        "    cur_y = []\n",
        "    for index in train_index_list[i]:\n",
        "      cur_x.append(train_x[index])\n",
        "      cur_y.append(train_y[index])\n",
        "    #cur_x, cur_y = (make_split_train_data_by_number(i, train_index_list, max_size))\n",
        "    new_x.append(np.array(cur_x))\n",
        "    new_y.append(np.array(cur_y))\n",
        "\n",
        "  return new_x, new_y\n",
        " \n",
        "def make_split_train_data_by_number(index_number, train_index_list, size=600):\n",
        "    if index_number != -1 :\n",
        "        random_index = np.random.randint(0, high=len(train_index_list[index_number]), size=min(size, len(train_index_list[index_number])))\n",
        "        s_train_x = []\n",
        "        s_train_y = []\n",
        "        for v in random_index:\n",
        "            s_train_x.append(train_x[train_index_list[index_number][v]])\n",
        "            s_train_y.append(train_y[train_index_list[index_number][v]])\n",
        "        return s_train_x, s_train_y\n",
        "    else:\n",
        "        return train_x, train_y\n",
        "\n",
        "# Balanced data \n",
        "def get_fixed_amount_balanced_data(num_clients, data_amount):\n",
        "  x,y = unison_shuffled_copies(train_x, train_y)\n",
        "  x = x[:num_clients*data_amount]\n",
        "  y = y[:num_clients*data_amount]\n",
        "  new_x = np.array_split(x, num_clients)\n",
        "  new_y = np.array_split(y, num_clients)  \n",
        "  return new_x, new_y\n",
        "\n",
        "# Imbalanced Data Distribution\n",
        "def get_imbalanced_data(num_clients):\n",
        "  random_data_amounts = np.random.randint(1, high=num_train_data_point//num_clients, size=num_clients, dtype=int)\n",
        "  print(random_data_amounts)\n",
        "  x,y = unison_shuffled_copies(train_x, train_y)\n",
        "  start = 0\n",
        "  new_x = []\n",
        "  new_y = []\n",
        "  for amount in random_data_amounts:\n",
        "    end = start + amount\n",
        "    current_x = x[start:end]\n",
        "    current_y = y[start:end]\n",
        "    start = end\n",
        "    new_x.append(current_x)\n",
        "    new_y.append(current_y)\n",
        "  return new_x, new_y\n",
        "\n",
        "# To be used for splitting the data\n",
        "#fed_x , fed_y = getBalancedData(num_clients)\n",
        "# fed_x , fed_y = split_data(\"BALANCED\", num_clients)\n",
        "# fed_x = np.array(fed_x)\n",
        "# fed_y = np.array(fed_y)\n",
        "# for y in fed_y:\n",
        "#   print(len(y))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKzPRICD2jdZ"
      },
      "source": [
        "def getModel():\n",
        "  model = tf.keras.models.Sequential([                                        \n",
        "    tf.keras.layers.LSTM(128, activation='tanh', input_shape=(INPUT_SHAPE), return_sequences=False),\n",
        "    tf.keras.layers.Dense(32, activation=\"tanh\", kernel_regularizer='l2'   ),\n",
        "    tf.keras.layers.Dense(32, activation=\"tanh\", kernel_regularizer='l2'   ),\n",
        "    # tf.keras.layers.LSTM(16,  activation='tanh',return_sequences=False, kernel_regularizer='l2', dropout=0.3),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\", kernel_regularizer='l2')])\n",
        "  return model\n",
        "\n",
        "def visualize_metrics(metrics, Loss=True):\n",
        "      # Get training and test loss histories\n",
        "  count = 0\n",
        "  acc = metrics[\"acc\"]\n",
        "  loss = metrics[\"loss\"]\n",
        "  \n",
        "  # # Create count of the number of epochs\n",
        "  epoch_count = range(1, len(acc) + 1)\n",
        "  # Visualize loss history\n",
        "  if Loss:\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.plot(epoch_count, loss, 'r--')\n",
        "    plt.legend(['Training Loss'])\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "  plt.subplot(2,1,2)\n",
        "  plt.plot(epoch_count, acc, 'b--')\n",
        "  plt.legend(['Accuracy'])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Acc')\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.show()\n",
        "\n",
        "def visualize_edge_metrics(server, Loss=False):\n",
        "  for edge_server in server.edge_servers:\n",
        "    print(edge_server.no)\n",
        "    visualize_metrics(edge_server.history, Loss=Loss)\n",
        "  "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BibHjSC-J2V"
      },
      "source": [
        "class Client():\n",
        "  def __init__(self, id, x, y):\n",
        "    self.id = id\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.amount_of_data = len(y)\n",
        "    self.model = getModel()\n",
        "    self.old_model = []\n",
        "    self.print_data_points()\n",
        "\n",
        "  def print_data_points(self):\n",
        "    print(\"CLIENT \", self.id, \"-------------------\")\n",
        "    print(\"data point number: \", len(self.x))\n",
        "    print(Counter(self.y.reshape(len(self.y,))))\n",
        "\n",
        "  def init_session(self, global_model):\n",
        "    self.model.set_weights( global_model )\n",
        "\n",
        "  def train_client( self, learning_rate, batch_size, local_epochs, global_model ):\n",
        "    #old_model = np.array( self.model.get_weights(), dtype=object )\n",
        "    updated_model = copy.deepcopy(global_model) \n",
        "    self.model.set_weights( updated_model )\n",
        "    optimizer = tf.keras.optimizers.SGD( lr=learning_rate )\n",
        "    self.model.compile( loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"] )\n",
        "    self.model.fit( self.x, self.y, epochs=local_epochs, batch_size=batch_size, shuffle=True, verbose=1 )\n",
        "    new_model = np.array( self.model.get_weights() , dtype=object )\n",
        "    #gradients = np.subtract( new_model , old_model )\n",
        "    return new_model\n",
        "\n",
        "class EdgeServer():\n",
        "  def __init__( self, client_list, client_indexes, global_model, no):\n",
        "    self.client_list = client_list\n",
        "    self.client_indexes = client_indexes\n",
        "    self.gradients = np.multiply( np.array(global_model ,dtype=object), 0 )\n",
        "    self.model_weights = np.array(global_model ,dtype=object)\n",
        "    self.num_clients = len(client_indexes)\n",
        "    self.history = {\"loss\" : [], \"acc\" :[], \"prec\" :[] , \"recall\" : [] }\n",
        "    self.model = getModel()\n",
        "    self.no = no\n",
        "    \n",
        "  \n",
        "  def train_slaves(self, learning_rate, batch_size, local_epochs, edge_interval, global_interval): # :)\n",
        "    for round in range(global_interval):\n",
        "      returned_models_total = np.multiply(self.gradients, 0)\n",
        "      client_models = []\n",
        "\n",
        "      for client in self.client_list:\n",
        "        client_model =  client.train_client(learning_rate, batch_size, edge_interval, self.model_weights)\n",
        "        returned_models_total = np.add( returned_models_total, client_model ) \n",
        "        client_models.append(client_model)\n",
        "\n",
        "      avg_model = np.divide( returned_models_total , self.num_clients )\n",
        "      Z = Utilities.ida_normalization_factor(client_models, avg_model)\n",
        "      ida_factors = Utilities.all_ida_coefficients(client_models, avg_model, Z)\n",
        "      weighted_total_client_models = np.multiply(self.gradients, 0)\n",
        "\n",
        "      for i in range(len(client_models)):\n",
        "        weighted_total_client_models = np.add(weighted_total_client_models, np.multiply( client_models[i] , ida_factors[i] ) )\n",
        "        \n",
        "\n",
        "      self.model_weights = weighted_total_client_models\n",
        "\n",
        "      self.model.set_weights(self.model_weights)\n",
        "\n",
        "      optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
        "      self.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,metrics=[\"accuracy\", \n",
        "               tf.keras.metrics.AUC(curve='PR', multi_label=True),\n",
        "               tf.keras.metrics.AUC(curve=\"ROC\"),\n",
        "               tf.keras.metrics.Precision(top_k=1),\n",
        "               tf.keras.metrics.Recall(top_k=1),\n",
        "               tf.keras.metrics.FalseNegatives(),\n",
        "               tf.keras.metrics.FalsePositives(),\n",
        "               ])\n",
        "      print( \"EDGE SERVER {} METRICS : \".format(self.no) )\n",
        "      current_state = self.model.evaluate( test_x , test_y )\n",
        "      self.history['loss'].append(current_state[0])\n",
        "      self.history['acc'].append(current_state[1])\n",
        "      self.history[\"prec\"].append(current_state[2])\n",
        "      self.history[\"recall\"].append(current_state[3])\n",
        "\n",
        "      return self.model_weights\n",
        "  \n",
        "class Server():\n",
        "  def __init__(self, num_clients, num_edge_servers, distribution, edge_interval, global_interval, load_weights=False, visualization_freq=30 ):\n",
        "    self.history = {\"loss\" : [], \"acc\" :[] }\n",
        "    self.model = getModel()\n",
        "    self.num_clients = num_clients\n",
        "    self.num_edge_servers = num_edge_servers\n",
        "    self.gradients = np.multiply( np.array(self.model.get_weights() ,dtype=object), 0 )\n",
        "    self.clients = []\n",
        "    self.edge_servers = []\n",
        "    self.visualization_freq = visualization_freq\n",
        "    self.curr_epoch = 0\n",
        "    self.distribution = distribution\n",
        "    self.edge_interval = edge_interval\n",
        "    self.global_interval = global_interval\n",
        "    self.init_clients()\n",
        "    self.init_edge_servers()\n",
        "\n",
        "  def train_slave_masters(self, learning_rate, batch_size, local_epochs, edge_interval, global_interval, federated_rounds, decay_rate):\n",
        "    # self.init_clients()\n",
        "    # self.init_edge_servers()\n",
        "\n",
        "    for round in range(federated_rounds):\n",
        "      returned_models = np.multiply(self.gradients, 0)\n",
        "      edge_models = [] \n",
        "      if round % 4 == 0 :\n",
        "        self.clear_backend()\n",
        "      \n",
        "      for edge_server in self.edge_servers: \n",
        "        edge_model = edge_server.train_slaves(learning_rate, batch_size, local_epochs, edge_interval, global_interval) \n",
        "        returned_models = np.add(returned_models, edge_model)\n",
        "        edge_models.append(edge_model)\n",
        "      \n",
        "      avg_model = np.divide( returned_models, len(self.edge_servers) )\n",
        "      Z = Utilities.ida_normalization_factor(edge_models, avg_model)\n",
        "      ida_factors = Utilities.all_ida_coefficients(edge_models, avg_model, Z)\n",
        "      updated_model = np.multiply(self.gradients, 0)\n",
        "      for i in range(len(edge_models)):\n",
        "        updated_model = np.add(updated_model, np.multiply( edge_models[i] , ida_factors[i] ) )\n",
        "      self.model.set_weights(updated_model)\n",
        "      optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
        "      self.model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\", \n",
        "               tf.keras.metrics.AUC(curve='PR', multi_label=True),\n",
        "               tf.keras.metrics.AUC(curve=\"ROC\"),\n",
        "               tf.keras.metrics.Precision(top_k=1),\n",
        "               tf.keras.metrics.Recall(top_k=1),\n",
        "               tf.keras.metrics.FalseNegatives(),\n",
        "               tf.keras.metrics.FalsePositives(),\n",
        "               ])\n",
        "      print(\"\"\".... EPOCH {} DONE ....\"\"\".format(round))\n",
        "      current_state = self.model.evaluate( train_x, train_y )\n",
        "      self.model.evaluate( test_x , test_y )\n",
        "      self.history['loss'].append(current_state[0])\n",
        "      self.history['acc'].append(current_state[1])\n",
        "      if round != 0 and round % self.visualization_freq == 0 : \n",
        "        visualize_metrics(self.history)\n",
        "      learning_rate = learning_rate  * decay_rate\n",
        "      \n",
        "  def init_clients(self):\n",
        "    weight_factors = []\n",
        "    total_data = 0\n",
        "    for i in range(self.num_clients):\n",
        "      self.clients.append( Client( i, fed_x[ i ], fed_y[ i ] ) )\n",
        "    \n",
        "    initial_model = np.array( self.model.get_weights(), dtype=object )\n",
        "    for client in self.clients:\n",
        "      client.init_session(initial_model)\n",
        "      #burda niye amount of data eklemişiz :\n",
        "      # FedAvg'dan kalma,  Gerek yok aslinda ama zararı da yok bundan sonraki 2 satırın\n",
        "      weight_factors.append(client.amount_of_data)\n",
        "      total_data += client.amount_of_data\n",
        "    self.weight_factors = np.array(weight_factors) / float(total_data)\n",
        "\n",
        "  def init_edge_servers(self):\n",
        "    edge = self.num_edge_servers\n",
        "    client_number = self.num_clients//self.num_edge_servers\n",
        "    for i in range(self.num_edge_servers):\n",
        "      self.edge_servers.append( EdgeServer( [], [], self.model.get_weights(), i+1 ) )\n",
        "    for i in range(self.num_clients):\n",
        "      edge_index = i//client_number\n",
        "      self.edge_servers[edge_index].client_list.append(self.clients[i])\n",
        "      self.edge_servers[edge_index].client_indexes.append(i)\n",
        "      self.edge_servers[edge_index].num_clients += 1\n",
        "    print( \"... EDGE SERVERS INITIALIZED ...\" )\n",
        "      \n",
        "  \n",
        "\n",
        "  def clear_backend(self):\n",
        "    print(\"CLEARING BACKEND....\")\n",
        "    saved_weights = self.model.get_weights() \n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "    self.model.set_weights(saved_weights)\n",
        "    del self.clients\n",
        "    self.clients = []\n",
        "    del self.edge_servers\n",
        "    self.edge_servers = []\n",
        "    self.init_clients()\n",
        "    self.init_edge_servers() \n",
        "    # for client in self.clients:\n",
        "      # client.model.set_weights(saved_weights)\n",
        "    print(\"BACKEND CLEARED!\")\n",
        "\n",
        "  "
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV8rZdfXM8Qe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0094a7ca-ca56-43cc-cdfb-bdacb0ca1dd4"
      },
      "source": [
        "# BALANCED CASE\n",
        "distribution = \"BALANCED\"\n",
        "fed_x , fed_y = split_data(distribution, num_clients)\n",
        "fed_x = np.array(fed_x)\n",
        "fed_y = np.array(fed_y)\n",
        "\n",
        "server1 = Server( num_clients, 2, distribution, edge_interval=1, global_interval=3, load_weights=False, visualization_freq=10)\n",
        "# server1.model.summary()\n",
        "server1.train_slave_masters(learning_rate, batch_size, local_epochs, 2, 3, federated_rounds, decay_rate)\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
        "server1.model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\", \n",
        "               tf.keras.metrics.AUC(curve='PR', multi_label=True),\n",
        "               tf.keras.metrics.AUC(curve=\"ROC\"),\n",
        "               tf.keras.metrics.Precision(top_k=1),\n",
        "               tf.keras.metrics.Recall(top_k=1),\n",
        "               tf.keras.metrics.FalseNegatives(),\n",
        "               tf.keras.metrics.FalsePositives(),\n",
        "               ])\n",
        "\n",
        "print(\"TEST SET EVALUATION\")\n",
        "print(server1.model.evaluate(test_x , test_y))\n",
        "print(server1.model.evaluate(train_x , train_y))\n",
        "visualize_metrics(server1.history)\n",
        "visualize_edge_metrics(server1)\n",
        "\n",
        "print(\"HISTORY : \")\n",
        "print(server1.model.history)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CLIENT  0 -------------------\n",
            "data point number:  1469\n",
            "Counter({0: 1273, 1: 196})\n",
            "CLIENT  1 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1276, 1: 192})\n",
            "CLIENT  2 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1274, 1: 194})\n",
            "CLIENT  3 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1287, 1: 181})\n",
            "CLIENT  4 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1277, 1: 191})\n",
            "CLIENT  5 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1267, 1: 201})\n",
            "CLIENT  6 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1261, 1: 207})\n",
            "CLIENT  7 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1259, 1: 209})\n",
            "CLIENT  8 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1244, 1: 224})\n",
            "CLIENT  9 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1276, 1: 192})\n",
            "... EDGE SERVERS INITIALIZED ...\n",
            "CLEARING BACKEND....\n",
            "CLIENT  0 -------------------\n",
            "data point number:  1469\n",
            "Counter({0: 1273, 1: 196})\n",
            "CLIENT  1 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1276, 1: 192})\n",
            "CLIENT  2 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1274, 1: 194})\n",
            "CLIENT  3 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1287, 1: 181})\n",
            "CLIENT  4 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1277, 1: 191})\n",
            "CLIENT  5 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1267, 1: 201})\n",
            "CLIENT  6 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1261, 1: 207})\n",
            "CLIENT  7 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1259, 1: 209})\n",
            "CLIENT  8 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1244, 1: 224})\n",
            "CLIENT  9 -------------------\n",
            "data point number:  1468\n",
            "Counter({0: 1276, 1: 192})\n",
            "... EDGE SERVERS INITIALIZED ...\n",
            "BACKEND CLEARED!\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-87ba1714141b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mserver1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mServer\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnum_clients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualization_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# server1.model.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mserver1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_slave_masters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfederated_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-5f09f9df943e>\u001b[0m in \u001b[0;36mtrain_slave_masters\u001b[0;34m(self, learning_rate, batch_size, local_epochs, edge_interval, global_interval, federated_rounds, decay_rate)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0medge_server\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_servers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0medge_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_slaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mreturned_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturned_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0medge_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-5f09f9df943e>\u001b[0m in \u001b[0;36mtrain_slaves\u001b[0;34m(self, learning_rate, batch_size, local_epochs, edge_interval, global_interval)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mclient_model\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mreturned_models_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreturned_models_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_model\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mclient_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-5f09f9df943e>\u001b[0m in \u001b[0;36mtrain_client\u001b[0;34m(self, learning_rate, batch_size, local_epochs, global_model)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#gradients = np.subtract( new_model , old_model )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-0x9KN4jhS5"
      },
      "source": [
        "\n",
        "print(\"TEST SET EVALUATION\")\n",
        "print(server1.model.evaluate(test_x , test_y))\n",
        "print(server1.model.evaluate(train_x , train_y))\n",
        "visualize_metrics(server1.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwDttngrt_tD"
      },
      "source": [
        "server1.train_slave_masters(learning_rate, batch_size, local_epochs, 15, 4, 1, decay_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVLQWhcubqaj"
      },
      "source": [
        "# SKEWED CASE\n",
        "distribution = \"SKEWED\"\n",
        "fed_x , fed_y = split_data(distribution, num_clients)\n",
        "fed_x = np.array(fed_x)\n",
        "fed_y = np.array(fed_y)\n",
        "\n",
        "server1 = Server(num_clients, distribution, load_weights=False)\n",
        "server1.init_clients()\n",
        "server1.model.summary()\n",
        "server1.train_slaves(learning_rate, batch_size, local_epochs, federated_rounds)\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
        "server1.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "print(\"TEST SET EVALUATION\")\n",
        "print(server1.model.evaluate(test_x , test_y))\n",
        "visualize_metrics(server1.history)\n",
        "\n",
        "print(\"HISTORY : \")\n",
        "print(server1.model.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3WT1huq2jfe"
      },
      "source": [
        "optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
        "server.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "a = server.model.evaluate(test_x , test_y)\n",
        "history = server.model.history\n",
        "print(a)\n",
        "print(history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkcIGsJd2jh4"
      },
      "source": [
        "# server.train_slaves(0.001, batch_size, local_epochs, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L73YfswG2jj-"
      },
      "source": [
        "# IMBALANCED CASE\n",
        "distribution = \"FIXED_SIZE_BALANCED_DATA\"\n",
        "fed_x , fed_y = split_data(distribution, num_clients)\n",
        "fed_x = np.array(fed_x)\n",
        "fed_y = np.array(fed_y)\n",
        "\n",
        "server2 = Server(num_clients, distribution, load_weights=load_weights)\n",
        "server2.init_clients()\n",
        "server2.model.summary()\n",
        "server2.train_slaves(learning_rate, batch_size, local_epochs, federated_rounds)\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
        "server2.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "print(\"TEST SET EVALUATION\")\n",
        "print(server2.model.evaluate(test_x , test_y))\n",
        "visualize_metrics(server2.history)\n",
        "\n",
        "print(\"HISTORY : \")\n",
        "print(server2.model.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-aGYxcVA6uz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbnkuKDU2jmW"
      },
      "source": [
        "# a = getModel()\n",
        "# a.compile(optimizer=tf.optimizers.SGD(), loss=\"sparse_categorical_crossentropy\")\n",
        "# a.fit(train_x, train_y, epochs=1, shuffle=True, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggm6WPxI2jov"
      },
      "source": [
        "shapes = [\n",
        "          [784, 10],\n",
        "          [10]\n",
        "]\n",
        "aa = np.array(server.model.get_weights(), dtype=object)\n",
        "\n",
        "def flatten_weights(weights, shape=shapes):\n",
        "  flat_ = []\n",
        "  for layer in weights:\n",
        "    flat_ = tf.concat([flat_, tf.reshape(layer , [-1, ]) ] , axis=0)\n",
        "  return flat_\n",
        "l = flatten_weights(aa)\n",
        "\n",
        "b = tf.Variable(l, trainable=True, dtype=tf.float32)\n",
        "\n",
        "tf.norm(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNvi5ez32jq7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}